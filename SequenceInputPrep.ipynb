{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Preparation\n",
    "This notebook is meant to prepare a data file for use in ML experiments and run an experiment.  The data file will be a CSV that is updated with a sequence group column and a label (target) column for a classification problem using machine learning models to predict the target.  The sequence group column will be used to group sequences that are related to each other.  The label column will be used to indicate the target value for each sequence.  The data file will be used to train and test machine learning models to predict the target value for new sequences.\n",
    "\n",
    "The steps outlined here will build upon one another so that the final data file will be ready for use in machine learning experiments.  The code should be run sequentially to prepare the data file.\n",
    "\n",
    "## File Preperation\n",
    "The basic steps for getting a file ready for the ML expermient are:\n",
    "1. Load the data file\n",
    "1. Filter the columns of interest which would include the features and the target (also known as the label)\n",
    "1. Normalize the data\n",
    "1. Create a sequence group column using a rolling window\n",
    "1. Shape the data\n",
    "1. Split the data, build, compile & train the model\n",
    "1. Evaluate the model\n",
    "\n",
    "### Data File Assumptions\n",
    "- The data file will be a CSV\n",
    "- The data file will have a header row\n",
    "- The data file will have a column that contains the target value\n",
    "- The data file will be in sequential chronological order\n",
    "- The data file will have a column that contains a date and time value to aide sequence grouping\n",
    "- The data file is already cleansed with regard to missing values and outliers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps 1 & 2: Load the Data File and Filter the Columns of Interest**\n",
    "- Update the file paths\n",
    "- Update the column names for the features and the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1 & 2\n",
    "# Declare the input & output file paths, the columns to write and the target column\n",
    "# perform the imports\n",
    "import pandas as pd\n",
    "\n",
    "file_in = './data/sample.csv'\n",
    "file_tmp = './tmp/sample.csv'\n",
    "file_out = './data_prod/sample.csv'\n",
    "file_training = './data_prod/sample_training.csv'\n",
    "file_testing = './data_prod/sample_testing.csv'\n",
    "\n",
    "columns_to_write = [\n",
    "    'date',\n",
    "    'reversal',\n",
    "    'adx',\n",
    "    'dmi',\n",
    "    'fisherstransform',\n",
    "    'fosc',\n",
    "    'linreg',\n",
    "    'linregintercept',\n",
    "    'linregslope',\n",
    "    'macd',\n",
    "    'macd_avg',\n",
    "    'macd_diff',\n",
    "    'parabolic_sar',\n",
    "    'rsi',\n",
    "    'rsi_avg',\n",
    "    'trendsequence']\n",
    "target_column = 'reversal'\n",
    "\n",
    "# Load the data from the input CSV file into a pandas dataframe\n",
    "df = pd.read_csv(file_in)\n",
    "\n",
    "# Convert 'date' column to datetime format for easier manipulation\n",
    "df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "# Save the modified dataframe with only the specified columns to a new CSV file\n",
    "#df.to_csv(file_tmp, index=False, columns=columns_to_write)\n",
    "df['reversal'] = df['reversal'].astype(int)\n",
    "df_filtered = df[columns_to_write]\n",
    "#df_filtered.to_csv(file_tmp, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Normalize the Data**\n",
    "\n",
    "This step will use the MinMaxScaler to normalize the data.  The MinMaxScaler will scale the data to a range of 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# List of features to exclude from normalization\n",
    "features_to_exclude = ['date', 'reversal']\n",
    "\n",
    "# Dynamically select features to normalize (all features except the ones to exclude)\n",
    "features_to_normalize = [col for col in df_filtered.columns if col not in features_to_exclude]\n",
    "\n",
    "# Initialize the Min-Max Scaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Fit the scaler to your data (for the features to be normalized)\n",
    "scaler.fit(df_filtered[features_to_normalize])\n",
    "\n",
    "# Transform the data using the fitted scaler\n",
    "df_normalized = df_filtered.copy()  # Create a copy of the DataFrame to keep the original data intact\n",
    "df_normalized[features_to_normalize] = scaler.transform(df_filtered[features_to_normalize])\n",
    "\n",
    "# df_normalized now contains the normalized data, excluding the specified features\n",
    "#df_normalized.to_csv(file_out, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: Create a Sequence Group Column Using a Rolling Window**\n",
    "\n",
    "This step will use a rolling window to create a sequence group column.  The sequence group column will be used to group sequences that are related to each other.  The rolling window will be based on a number of records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4045\n"
     ]
    }
   ],
   "source": [
    "# STEP 4\n",
    "rows_per_group = 5\n",
    "\n",
    "df_normalized['logical_date'] = df_normalized['date'].dt.date\n",
    "# Initialize an empty DataFrame to hold the final results\n",
    "final_df = pd.DataFrame()\n",
    "\n",
    "# Loop over the DataFrame to create rolling windows\n",
    "for start in range(len(df_normalized) - rows_per_group):\n",
    "    window = df_normalized.iloc[start:start + rows_per_group]\n",
    "    # Check if all the dates in the window are the same\n",
    "    if len(set(window['logical_date'])) == 1:\n",
    "        sequence_group = start + 1\n",
    "        # Check if the next record exists and is on the same logical date\n",
    "        if start + rows_per_group < len(df_normalized) and window.iloc[-1]['logical_date'] == df_normalized.iloc[start + rows_per_group]['logical_date']:\n",
    "            future_reversal = df_normalized.iloc[start + rows_per_group]['reversal']\n",
    "        else:\n",
    "            future_reversal = None  # Set to None if there's no next record or it's on a different date\n",
    "        \n",
    "        window_copy = window.copy()\n",
    "        window_copy['sequence_group'] = sequence_group\n",
    "        window_copy['future_reversal'] = future_reversal\n",
    "        final_df = pd.concat([final_df, window_copy], ignore_index=True)\n",
    "\n",
    "# Filter out any sequence groups that don't have a future_reversal (indicating the next record was on a different day)\n",
    "final_df = final_df.dropna(subset=['future_reversal'])\n",
    "\n",
    "# Drop the date and convert all columns to float\n",
    "final_df = final_df.drop(['date','logical_date'], axis=1)\n",
    "final_df = final_df.astype(float)\n",
    "\n",
    "# Write the final DataFrame to a new CSV file\n",
    "print(len(final_df))\n",
    "#print(final_df.head())\n",
    "#final_df.to_csv(file_tmp, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5: Shape the Data**\n",
    "Shape the data for RNN input, which requires a 3D shape [samples, timesteps, features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Drop the 'sequence_group' column and separate features and labels\n",
    "X = final_df.drop(['sequence_group', 'future_reversal'], axis=1)\n",
    "y = final_df['future_reversal']\n",
    "\n",
    "# Since the data is already grouped, reshape it to fit the RNN input shape\n",
    "num_features = X.shape[1]\n",
    "num_sequences = len(final_df) // rows_per_group \n",
    "\n",
    "X_reshaped = X.values.reshape((num_sequences, rows_per_group, num_features))\n",
    "y_reshaped = y.values.reshape((num_sequences, rows_per_group))[:, 0]  # Take the first label of each sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6: Split the Data, Build, Compile & Train the Model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-04 13:41:37.027202: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-04 13:41:42.188793: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f93ccd49f40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-02-04 13:41:42.188884: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1070, Compute Capability 6.1\n",
      "2024-02-04 13:41:42.355903: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-02-04 13:41:42.656410: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1707072102.972153   17901 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 7s 65ms/step - loss: 0.7018 - accuracy: 0.6151 - val_loss: 0.6691 - val_accuracy: 0.6308\n",
      "Epoch 2/10\n",
      "17/17 [==============================] - 1s 31ms/step - loss: 0.6895 - accuracy: 0.6015 - val_loss: 0.6666 - val_accuracy: 0.6308\n",
      "Epoch 3/10\n",
      "17/17 [==============================] - 0s 28ms/step - loss: 0.6696 - accuracy: 0.6383 - val_loss: 0.6714 - val_accuracy: 0.6308\n",
      "Epoch 4/10\n",
      "17/17 [==============================] - 0s 29ms/step - loss: 0.6682 - accuracy: 0.6576 - val_loss: 0.6664 - val_accuracy: 0.6308\n",
      "Epoch 5/10\n",
      "17/17 [==============================] - 1s 45ms/step - loss: 0.6723 - accuracy: 0.6286 - val_loss: 0.6660 - val_accuracy: 0.6308\n",
      "Epoch 6/10\n",
      "17/17 [==============================] - 1s 34ms/step - loss: 0.6563 - accuracy: 0.6402 - val_loss: 0.6689 - val_accuracy: 0.6308\n",
      "Epoch 7/10\n",
      "17/17 [==============================] - 1s 36ms/step - loss: 0.6455 - accuracy: 0.6557 - val_loss: 0.6772 - val_accuracy: 0.6308\n",
      "Epoch 8/10\n",
      "17/17 [==============================] - 1s 39ms/step - loss: 0.6470 - accuracy: 0.6402 - val_loss: 0.6656 - val_accuracy: 0.6308\n",
      "Epoch 9/10\n",
      "17/17 [==============================] - 1s 37ms/step - loss: 0.6516 - accuracy: 0.6634 - val_loss: 0.6723 - val_accuracy: 0.6308\n",
      "Epoch 10/10\n",
      "17/17 [==============================] - 1s 30ms/step - loss: 0.6584 - accuracy: 0.6306 - val_loss: 0.6684 - val_accuracy: 0.6308\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.6349 - accuracy: 0.6605\n",
      "Test Accuracy: 0.6605\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_reshaped, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the RNN model\n",
    "model = Sequential([\n",
    "    SimpleRNN(50, input_shape=(rows_per_group, num_features), return_sequences=False),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 7: Evaluate the Model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 26ms/step - loss: 0.6349 - accuracy: 0.6605\n",
      "Test Accuracy: 0.6605\n",
      "6/6 [==============================] - 0s 8ms/step\n",
      "\n",
      "             |      Predicted       |\n",
      "+--------------+--------------+--------------+\n",
      "|              | Not Reversed |   Reversed   |\n",
      "+--------------+--------------+--------------+\n",
      "| Actual     |     107      |      0       |\n",
      "+--------------+--------------+--------------+\n",
      "| Not Reversed |      55      |      0       |\n",
      "+--------------+--------------+--------------+\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Assuming y_test contains the true labels and y_pred_classes contains your model predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = (y_pred > 0.5).astype(\"int32\")  # Convert probabilities to binary predictions\n",
    "\n",
    "# Define the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_classes)\n",
    "\n",
    "# Labels for rows and columns\n",
    "labels = ['Not Reversed', 'Reversed']\n",
    "\n",
    "# Prepare the header and column names with separators\n",
    "header = f\"{'|':>14} {'Predicted':^20} |\"\n",
    "column_names = f\"| {'':>12} | {labels[0]:^12} | {labels[1]:^12} |\"\n",
    "\n",
    "# Separator for clarity between header and rows\n",
    "separator = \"+\" + \"-\"*14 + \"+\" + \"-\"*14 + \"+\" + \"-\"*14 + \"+\"\n",
    "\n",
    "# Data rows with separators\n",
    "row1 = f\"| {'Actual':<10} | {cm[0][0]:^12} | {cm[0][1]:^12} |\"\n",
    "row2 = f\"| {labels[0]:<10} | {cm[1][0]:^12} | {cm[1][1]:^12} |\"\n",
    "\n",
    "# Assemble the full table\n",
    "table = f\"\\n{header}\\n{separator}\\n{column_names}\\n{separator}\\n{row1}\\n{separator}\\n{row2}\\n{separator}\"\n",
    "\n",
    "# Print the table\n",
    "print(table)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
