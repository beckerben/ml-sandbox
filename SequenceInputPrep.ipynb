{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Preparation\n",
    "This notebook is meant to prepare a data file for use in ML experiments.  The data file will be a CSV that is updated with a sequence group column and a label (target) column for a classification problem using machine learning models to predict the target.  The sequence group column will be used to group sequences that are related to each other.  The label column will be used to indicate the target value for each sequence.  The data file will be used to train and test machine learning models to predict the target value for new sequences.\n",
    "\n",
    "The steps outlined here will build upon one another so that the final data file will be ready for use in machine learning experiments.  The code should be run sequentially to prepare the data file.\n",
    "\n",
    "## File Preperation\n",
    "The basic steps for getting a file ready for the ML expermient are:\n",
    "1. Load the data file\n",
    "1. Filter the columns of interest which would include the features and the target (also known as the label)\n",
    "1. Normalize the data\n",
    "1. Create a sequence group column using a rolling window\n",
    "1. Split the data into a training set and a test set\n",
    "\n",
    "### Data File Assumptions\n",
    "- The data file will be a CSV\n",
    "- The data file will have a header row\n",
    "- The data file will have a column that contains the target value\n",
    "- The data file will be in sequential chronological order\n",
    "- The data file will have a column that contains a date and time value to aide sequence grouping\n",
    "- The data file is already cleansed with regard to missing values and outliers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps 1 & 2: Load the Data File and Filter the Columns of Interest**\n",
    "- Update the file paths\n",
    "- Update the column names for the features and the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1 & 2\n",
    "# Declare the input & output file paths, the columns to write and the target column\n",
    "# perform the imports\n",
    "import pandas as pd\n",
    "\n",
    "file_in = './data/sample.csv'\n",
    "file_tmp = './tmp/sample.csv'\n",
    "file_out = './data_prod/sample.csv'\n",
    "file_training = './data_prod/sample_training.csv'\n",
    "file_testing = './data_prod/sample_testing.csv'\n",
    "\n",
    "columns_to_write = [\n",
    "    'date',\n",
    "    'reversal',\n",
    "    'adx',\n",
    "    'dmi',\n",
    "    'fisherstransform',\n",
    "    'fosc',\n",
    "    'linreg',\n",
    "    'linregintercept',\n",
    "    'linregslope',\n",
    "    'macd',\n",
    "    'macd_avg',\n",
    "    'macd_diff',\n",
    "    'parabolic_sar',\n",
    "    'rsi',\n",
    "    'rsi_avg',\n",
    "    'trendsequence']\n",
    "target_column = 'reversal'\n",
    "\n",
    "# Load the data from the input CSV file into a pandas dataframe\n",
    "df = pd.read_csv(file_in)\n",
    "\n",
    "# Convert 'date' column to datetime format for easier manipulation\n",
    "df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "# Save the modified dataframe with only the specified columns to a new CSV file\n",
    "#df.to_csv(file_tmp, index=False, columns=columns_to_write)\n",
    "df['reversal'] = df['reversal'].astype(int)\n",
    "df_filtered = df[columns_to_write]\n",
    "#df_filtered.to_csv(file_tmp, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Normalize the Data**\n",
    "\n",
    "This step will use the MinMaxScaler to normalize the data.  The MinMaxScaler will scale the data to a range of 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# List of features to exclude from normalization\n",
    "features_to_exclude = ['date', 'reversal']\n",
    "\n",
    "# Dynamically select features to normalize (all features except the ones to exclude)\n",
    "features_to_normalize = [col for col in df_filtered.columns if col not in features_to_exclude]\n",
    "\n",
    "# Initialize the Min-Max Scaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Fit the scaler to your data (for the features to be normalized)\n",
    "scaler.fit(df_filtered[features_to_normalize])\n",
    "\n",
    "# Transform the data using the fitted scaler\n",
    "df_normalized = df_filtered.copy()  # Create a copy of the DataFrame to keep the original data intact\n",
    "df_normalized[features_to_normalize] = scaler.transform(df_filtered[features_to_normalize])\n",
    "\n",
    "# df_normalized now contains the normalized data, excluding the specified features\n",
    "#df_normalized.to_csv(file_out, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: Create a Sequence Group Column Using a Rolling Window**\n",
    "\n",
    "This step will use a rolling window to create a sequence group column.  The sequence group column will be used to group sequences that are related to each other.  The rolling window will be based on a number of records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4\n",
    "rows_per_group = 5\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.read_csv(file_out)\n",
    "\n",
    "# Convert 'date' column to datetime format for easier manipulation\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['logical_date'] = df['date'].dt.date\n",
    "\n",
    "# Initialize an empty DataFrame to hold the final results\n",
    "final_df = pd.DataFrame()\n",
    "\n",
    "# Loop over the DataFrame to create rolling windows\n",
    "for start in range(len(df) - rows_per_group):\n",
    "    window = df.iloc[start:start + rows_per_group]\n",
    "    # Check if all the dates in the window are the same\n",
    "    if len(set(window['logical_date'])) == 1:\n",
    "        sequence_group = start + 1\n",
    "        # Check if the next record exists and is on the same logical date\n",
    "        if start + rows_per_group < len(df) and window.iloc[-1]['logical_date'] == df.iloc[start + rows_per_group]['logical_date']:\n",
    "            future_reversal = df.iloc[start + rows_per_group]['reversal']\n",
    "        else:\n",
    "            future_reversal = None  # Set to None if there's no next record or it's on a different date\n",
    "        \n",
    "        window_copy = window.copy()\n",
    "        window_copy['sequence_group'] = sequence_group\n",
    "        window_copy['future_reversal'] = future_reversal\n",
    "        final_df = pd.concat([final_df, window_copy], ignore_index=True)\n",
    "\n",
    "# Filter out any sequence groups that don't have a future_reversal (indicating the next record was on a different day)\n",
    "final_df = final_df.dropna(subset=['future_reversal'])\n",
    "\n",
    "#final_df.to_csv(file_tmp, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5: Split the Data into a Training Set and a Test Set**\n",
    "This step will split the data into a training set and a test set.  The training set will be used to train the machine learning models.  The test set will be used to evaluate the performance of the machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "# Define the GroupShuffleSplit splitter\n",
    "# test_size can be a float representing the proportion of the dataset to include in the test split\n",
    "gss = GroupShuffleSplit(test_size=0.3, n_splits=1, random_state=42)\n",
    "\n",
    "# The split method requires the features, the target, and the groups.\n",
    "# The target will be the 'future_reversal' column from your DataFrame.\n",
    "# Groups will be the 'sequence_group' column from your DataFrame.\n",
    "X = final_df.drop(columns=['future_reversal'])  # Features excluding the target\n",
    "y = final_df['future_reversal']  # Target\n",
    "groups = final_df['sequence_group']  # Groups\n",
    "\n",
    "# Perform the split\n",
    "for train_idx, test_idx in gss.split(X, y, groups=groups):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "# X_train and y_train now contain the training feature set and target set, respectively\n",
    "# X_test and y_test contain the testing feature set and target set, respectively\n",
    "\n",
    "# If you need the whole rows in the train/test sets you can do:\n",
    "train_df = final_df.iloc[train_idx]\n",
    "test_df = final_df.iloc[test_idx]\n",
    "\n",
    "# Output the first few rows of the train and test sets to verify\n",
    "train_df.to_csv(file_training, index=False)\n",
    "test_df.to_csv(file_testing, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type bool).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 15\u001b[0m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential([\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Embedding layer if your input data is not already vectorized. Adjust input_dim and output_dim as needed.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Embedding(input_dim=num_features, output_dim=128, input_length=max_sequence_length),\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     Dense(\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Use 'softmax' for multi-class classification.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m ])\n\u001b[1;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])  \u001b[38;5;66;03m# Use 'categorical_crossentropy' for multi-class.\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/proj/ml-sandbox/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/proj/ml-sandbox/.venv/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:103\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    102\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type bool)."
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Define the RNN model\n",
    "model = Sequential([\n",
    "    # Embedding layer if your input data is not already vectorized. Adjust input_dim and output_dim as needed.\n",
    "    # Embedding(input_dim=num_features, output_dim=128, input_length=max_sequence_length),\n",
    "    LSTM(64, return_sequences=False),  # Adjust the number of units based on your dataset complexity.\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])  # Use 'categorical_crossentropy' for multi-class.\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
