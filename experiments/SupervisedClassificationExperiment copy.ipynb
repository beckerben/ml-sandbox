{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Experiment using Supervised ML\n",
    "\n",
    "This notebook is meant to prepare a data file and conduct machine learning experiments using traditional supervised leraning models (random forest, decision tree, SVM, etc.).  The models will be used for binary classification.\n",
    "\n",
    "The data file will be a CSV that is updated with a sequence group column and a label (target) column for a classification problem using machine learning models to predict the target.  The \"sequence_group\" column will be used to group sequences that are related to each other.  The label column will be used to indicate the target value for each sequence.  The data file will be used to train and test machine learning models to predict the target value for new sequences.\n",
    "\n",
    "The steps outlined here will build upon one another and should be run sequentially so that the final data file will be processed using a number of different models.\n",
    "\n",
    "I highly recommend the use of a GPU for this experiment.  The use of a GPU will greatly reduce the time it takes to train the models.\n",
    "\n",
    "## File Preperation\n",
    "\n",
    "The basic steps for getting a file ready for the ML expermient are:\n",
    "\n",
    "1. Load the data file\n",
    "1. Filter the columns of interest which would include the features and the target (also known as the label)\n",
    "1. Normalize the data\n",
    "1. Create the target column lagged by one period\n",
    "1. Split the data, build, compile, train & evaluate the model\n",
    "\n",
    "The data file has been created using NinjaTrader 8 and is a CSV file.  The rows represent renko bars.  The other features represent the indicators which were used with defaults.  \n",
    "\n",
    "**Important**: The indicators used here are not good features for this problem because they are being used on a chart type and at a granularity that is not typical for the indicator.  The indicators are being used to demonstrate the process of preparing the data file and conducting the machine learning experiments.  The utility which was used to create the data file is the \"Exporter\" strategy which I authored in my NinjaTrader repository on GitHub.  I have included a sample data file in the /data directory using NQ 30 tick renko bars.\n",
    "\n",
    "**Note**: Any indicator which is a \"price\" type indicator has been converted to be a percentage difference from the \"close\" price of the related bar. This is to make the indicator values more consistent across different instruments and time frames.\n",
    "\n",
    "### Data File Assumptions\n",
    "\n",
    "- The data file will be a CSV\n",
    "- The data file will have a header row\n",
    "- The data file will have a column that contains the target value and it must be a binary value\n",
    "- The data file will be in sequential chronological order\n",
    "- The data file will have a column that contains a date and time value to aide sequence grouping\n",
    "- The data file is already cleansed with regard to missing values and outliers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps 1 & 2: Load the Data File and Filter the Columns of Interest**\n",
    "- Update the file paths\n",
    "- Update the column names for the features and the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14122/1448950857.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "# STEP 1 & 2\n",
    "# Declare the input & output file paths, the columns to write and the target column\n",
    "# perform the imports\n",
    "import pandas as pd\n",
    "\n",
    "file_in = '../data_prod/NQ0324_30TICK_07-16_20240203175932.csv'\n",
    "file_tmp = '../tmp/NQ0324_30TICK_07-16_20240203175932.csv'\n",
    "file_out = '../data_prod/NQ0324_30TICK_07-16_20240203175932.csv'\n",
    "file_training = '../data_prod/NQ0324_30TICK_07-16_20240203175932.csv'\n",
    "file_testing = '../data_prod/NQ0324_30TICK_07-16_20240203175932.csv'\n",
    "\n",
    "columns_to_write = [\n",
    "    'date',\n",
    "    'higherclose',\n",
    "    'reversal',\n",
    "    'trendsequence',\n",
    "    'adl',\n",
    "    'adx',\n",
    "    'adxr',\n",
    "    'apz_lower',\n",
    "    'apz_upper',\n",
    "    'aroonoscillator',\n",
    "    'atr',\n",
    "    'bollinger_lower',\n",
    "    'bollinger_middle',\n",
    "    'bollinger_upper',\n",
    "    'bop',\n",
    "    'camarilla_r1',\n",
    "    'camarilla_r2',\n",
    "    'camarilla_r3',\n",
    "    'camarilla_r4',\n",
    "    'camarilla_s1',\n",
    "    'camarilla_s2',\n",
    "    'camarilla_s3',\n",
    "    'camarilla_s4',\n",
    "    'cci',\n",
    "    'chaikinmoneyflow',\n",
    "    'chaikinoscillator',\n",
    "    'chaikinvolatility',\n",
    "    'choppinessindex',\n",
    "    'cmo',\n",
    "    'currentday_open',\n",
    "    'currentday_low',\n",
    "    'currentday_high',\n",
    "    'disparityindex',\n",
    "    'dm_diplus',\n",
    "    'dm_diminus',\n",
    "    'dmi',\n",
    "    'dmindex',\n",
    "    'donchian_lower',\n",
    "    'donchian_mean',\n",
    "    'donchian_upper',\n",
    "    'doublestochastics_k',\n",
    "    'easeofmovement',\n",
    "    'fibonacci_pp',\n",
    "    'fibonacci_r1',\n",
    "    'fibonacci_r2',\n",
    "    'fibonacci_r3',\n",
    "    'fibonacci_s1',\n",
    "    'fibonacci_s2',\n",
    "    'fibonacci_s3',\n",
    "    'fisherstransform',\n",
    "    'fosc',\n",
    "    'kama',\n",
    "    'keltner_lower',\n",
    "    'keltner_mean',\n",
    "    'keltner_upper',\n",
    "    'linreg',\n",
    "    'linregintercept',\n",
    "    'linregslope',\n",
    "    'macd',\n",
    "    'macd_avg',\n",
    "    'macd_diff',\n",
    "    'mama_default',\n",
    "    'mama_kama',\n",
    "    'mfi',\n",
    "    'momentum',\n",
    "    'moneyflowoscillator',\n",
    "    'orderflowcumulativedelta_deltaopen',\n",
    "    'orderflowcumulativedelta_deltaclose',\n",
    "    'orderflowcumulativedelta_deltahigh',\n",
    "    'orderflowcumulativedelta_deltalow',\n",
    "    'orderflowvwap_vwap',\n",
    "    'orderflowvwap_s1_lower',\n",
    "    'orderflowvwap_s1_higher',\n",
    "    'orderflowvwap_s2_lower',\n",
    "    'orderflowvwap_s2_higher',\n",
    "    'orderflowvwap_s3_lower',\n",
    "    'orderflowvwap_s3_higher',\n",
    "    'parabolic_sar',\n",
    "    'pfe',\n",
    "    'ppo',\n",
    "    'priceoscillator',\n",
    "    'psychologicalline',\n",
    "    'rsquared',\n",
    "    'relativevigorindex',\n",
    "    'rind',\n",
    "    'roc',\n",
    "    'rsi',\n",
    "    'rsi_avg',\n",
    "    'rss',\n",
    "    'rvi',\n",
    "    'stddev',\n",
    "    'stochrsi',\n",
    "    'stochastics_d',\n",
    "    'stochastics_k',\n",
    "    'stochasticsfast_d',\n",
    "    'stochasticsfast_k',\n",
    "    'trix',\n",
    "    'trix_signal',\n",
    "    'tsf',\n",
    "    'tsi',\n",
    "    'ultimateoscillator',\n",
    "    'vortex_viplus',\n",
    "    'vortex_viminus',\n",
    "    'volma',\n",
    "    'volume_oscillator',\n",
    "    'vroc',\n",
    "    'williamsr',\n",
    "    'wisemanawesomeoscillator',\n",
    "    'woodiescci',\n",
    "    'woodiescci_turbo',\n",
    "    'woodiespivot_pp',\n",
    "    'woodiespivot_r1',\n",
    "    'woodiespivot_r2',\n",
    "    'woodiespivot_s1',\n",
    "    'woodiespivot_s2'\n",
    "    ]\n",
    "\n",
    "group_helper = 'date' # This is the column that will be used to group the data and must be a datetime column in the format '%Y-%m-%d %H:%M:%S.%f'\n",
    "target_column = 'higherclose' # This is the column that will be used as the target column for the model and must be a binary column\n",
    "\n",
    "# Load the data from the input CSV file into a pandas dataframe\n",
    "df = pd.read_csv(file_in)\n",
    "\n",
    "# Convert 'date' column to datetime format for easier manipulation\n",
    "df[group_helper] = pd.to_datetime(df[group_helper], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "# Save the modified dataframe with only the specified columns to a new CSV file\n",
    "#df.to_csv(file_tmp, index=False, columns=columns_to_write)\n",
    "df[target_column] = df[target_column].astype(int)\n",
    "df_filtered = df[columns_to_write]\n",
    "#df_filtered.to_csv(file_tmp, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Normalize the Data**\n",
    "\n",
    "This step will use the MinMaxScaler to normalize the data.  The MinMaxScaler will scale the data to a range of 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# List of features to exclude from normalization\n",
    "features_to_exclude = [group_helper, target_column]\n",
    "\n",
    "# Dynamically select features to normalize (all features except the ones to exclude)\n",
    "features_to_normalize = [col for col in df_filtered.columns if col not in features_to_exclude]\n",
    "\n",
    "# Initialize the Scaler\n",
    "scaler = RobustScaler() # AKA Z-score normalization, better at handling outliers\n",
    "\n",
    "# Fit the scaler to the data (for the features to be normalized)\n",
    "scaler.fit(df_filtered[features_to_normalize])\n",
    "\n",
    "# Transform the data using the fitted scaler\n",
    "df_normalized = df_filtered.copy()  # Create a copy of the DataFrame to keep the original data intact\n",
    "df_normalized[features_to_normalize] = scaler.transform(df_filtered[features_to_normalize])\n",
    "\n",
    "#df_normalized.to_csv(file_out, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: Create the Target Column Lagged by One Period**\n",
    "\n",
    "This step will use the shift function to look ahead at the target_column to pull the value from the future onto the prior record.  This will be the \"future_target_column\" of interest for predictions.  Unlike RNN models, traditional ML models require the data to be in a tabular format.  This means that the data must be in a table with rows and columns (2D)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DateTime: 2024-02-05 23:47:52.361\n",
      "Record Count: 20748\n",
      "DateTime: 2024-02-05 23:47:52.457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14122/2465262045.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_normalized[f'future_{target_column}'] = df_normalized[target_column].shift(-1).astype('Int64')\n"
     ]
    }
   ],
   "source": [
    "# STEP 4\n",
    "from datetime import datetime\n",
    "\n",
    "print(f\"DateTime: {datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}\")\n",
    "\n",
    "df_normalized[f'future_{target_column}'] = df_normalized[target_column].shift(-1).astype('Int64')\n",
    "\n",
    "# Remove any rows with NaN values in the future target column\n",
    "df_normalized.dropna(subset=[f'future_{target_column}'], inplace=True)\n",
    "\n",
    "# Write the final DataFrame to a new CSV file\n",
    "print(f\"Record Count: {len(df_normalized)}\")\n",
    "#df_normalized.to_csv(file_tmp, index=False)\n",
    "print(f\"DateTime: {datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5: Split the Data, Build, Compile, Train & Evaluate the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, confusion_matrix, classification_report\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XGBClassifier\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightgbm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LGBMClassifier\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcatboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CatBoostClassifier\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "future_target_column = f'future_{target_column}'\n",
    "\n",
    "# Splitting the DataFrame into features (X) and target (y)\n",
    "X = df_normalized.drop(columns=[future_target_column])  # Features\n",
    "y = df_normalized[future_target_column]  # Target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "models = [\n",
    "    ('Logistic Regression', LogisticRegression(max_iter=200)),\n",
    "    ('Ridge Classifier', RidgeClassifier()),\n",
    "    ('Support Vector Classification', SVC()),\n",
    "    ('Linear Support Vector Classification', LinearSVC()),\n",
    "    ('Decision Tree', DecisionTreeClassifier()),\n",
    "    ('Random Forest', RandomForestClassifier(n_estimators=100)),\n",
    "    ('K-Neighbors Classifier', KNeighborsClassifier()),\n",
    "    ('Gaussian Naive Bayes', GaussianNB()),\n",
    "    ('Multinomial Naive Bayes', MultinomialNB()),\n",
    "    ('Bernoulli Naive Bayes', BernoulliNB()),\n",
    "    ('AdaBoost Classifier', AdaBoostClassifier(n_estimators=100)),\n",
    "    ('Gradient Boosting Classifier', GradientBoostingClassifier(n_estimators=100)),\n",
    "    ('Extra Trees Classifier', ExtraTreesClassifier(n_estimators=100)),\n",
    "    ('Bagging Classifier', BaggingClassifier(n_estimators=100)),\n",
    "    ('SGD Classifier', SGDClassifier(max_iter=1000, tol=1e-3))\n",
    "    #('XGBoost Classifier', XGBClassifier(use_label_encoder=False, eval_metric='logloss')),\n",
    "    #('LightGBM Classifier', LGBMClassifier()),\n",
    "    #('CatBoost Classifier', CatBoostClassifier(verbose=0))  # verbose=0 to keep the output clean\n",
    "]\n",
    "\n",
    "\n",
    "# Function to train and evaluate models\n",
    "def train_and_evaluate(models, X_train, X_test, y_train, y_test):\n",
    "    for name, model in models:\n",
    "        # Train model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict on testing set\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Evaluate model\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        print(f\"DateTime: {datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}\")\n",
    "        print(f\"Model: {name}\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(\"Classification Report:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(conf_matrix)\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        fig, ax = plt.subplots()\n",
    "        img = ax.matshow(conf_matrix, cmap=plt.cm.Blues)\n",
    "        plt.title(f'Confusion Matrix for {name}')\n",
    "        fig.colorbar(img)\n",
    "        ax.set_xticks(np.arange(len(np.unique(y))))\n",
    "        ax.set_yticks(np.arange(len(np.unique(y))))\n",
    "        # Assuming y contains integers as labels. Adjust if y contains other types.\n",
    "        ax.set_xticklabels(np.unique(y))\n",
    "        ax.set_yticklabels(np.unique(y))\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.show()\n",
    "\n",
    "# Train and evaluate each model\n",
    "train_and_evaluate(models, X_train, X_test, y_train, y_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
