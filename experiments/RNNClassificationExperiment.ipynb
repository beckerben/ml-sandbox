{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Experiment using Recurrent Neural Networks\n",
    "\n",
    "This notebook is meant to prepare a data file and conduct machine learning experiments using Neural Networks, specifically recurrent neural networks which support memory (RNN, LSTM, etc.).  The Neural Network models will be used for binary classification.\n",
    "\n",
    "The data file will be a CSV that is updated with a sequence group column and a label (target) column for a classification problem using machine learning models to predict the target.  The \"sequence_group\" column will be used to group sequences that are related to each other.  The label column will be used to indicate the target value for each sequence.  The data file will be used to train and test machine learning models to predict the target value for new sequences.\n",
    "\n",
    "The steps outlined here will build upon one another and should be run sequentially so that the final data file will be processed using a number of different Neural Network models.\n",
    "\n",
    "I highly recommend the use of a GPU for this experiment.  The use of a GPU will greatly reduce the time it takes to train the models.\n",
    "\n",
    "## File Preperation\n",
    "\n",
    "The basic steps for getting a file ready for the ML expermient are:\n",
    "\n",
    "1. Load the data file\n",
    "1. Filter the columns of interest which would include the features and the target (also known as the label)\n",
    "1. Normalize the data\n",
    "1. Create a sequence group column using a rolling window\n",
    "1. Shape the data\n",
    "1. Split the data, build, compile, train & evaluate the model\n",
    "\n",
    "The data file has been created using NinjaTrader 8 and is a CSV file.  The rows represent renko bars.  The other features represent the indicators which were used with defaults.  \n",
    "\n",
    "**Important**: The indicators used here are not good features for this problem because they are being used on a chart type and at a granularity that is not typical for the indicator.  The indicators are being used to demonstrate the process of preparing the data file and conducting the machine learning experiments.  The utility which was used to create the data file is the \"Exporter\" strategy which I authored in my NinjaTrader repository on GitHub.  I have included a sample data file in the /data directory using NQ 30 tick renko bars.\n",
    "\n",
    "**Note**: Any indicator which is a \"price\" type indicator has been converted to be a percentage difference from the \"close\" price of the related bar. This is to make the indicator values more consistent across different instruments and time frames.\n",
    "\n",
    "### Data File Assumptions\n",
    "\n",
    "- The data file will be a CSV\n",
    "- The data file will have a header row\n",
    "- The data file will have a column that contains the target value and it must be a binary value\n",
    "- The data file will be in sequential chronological order\n",
    "- The data file will have a column that contains a date and time value to aide sequence grouping\n",
    "- The data file is already cleansed with regard to missing values and outliers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps 1 & 2: Load the Data File and Filter the Columns of Interest**\n",
    "- Update the file paths\n",
    "- Update the column names for the features and the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2216/1448950857.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "# STEP 1 & 2\n",
    "# Declare the input & output file paths, the columns to write and the target column\n",
    "# perform the imports\n",
    "import pandas as pd\n",
    "\n",
    "file_in = '../data_prod/NQ0324_30TICK_07-16_20240203175932.csv'\n",
    "file_tmp = '../tmp/NQ0324_30TICK_07-16_20240203175932.csv'\n",
    "file_out = '../data_prod/NQ0324_30TICK_07-16_20240203175932.csv'\n",
    "file_training = '../data_prod/NQ0324_30TICK_07-16_20240203175932.csv'\n",
    "file_testing = '../data_prod/NQ0324_30TICK_07-16_20240203175932.csv'\n",
    "\n",
    "columns_to_write = [\n",
    "    'date',\n",
    "    'higherclose',\n",
    "    'reversal',\n",
    "    'trendsequence',\n",
    "    'adl',\n",
    "    'adx',\n",
    "    'adxr',\n",
    "    'apz_lower',\n",
    "    'apz_upper',\n",
    "    'aroonoscillator',\n",
    "    'atr',\n",
    "    'bollinger_lower',\n",
    "    'bollinger_middle',\n",
    "    'bollinger_upper',\n",
    "    'bop',\n",
    "    'camarilla_r1',\n",
    "    'camarilla_r2',\n",
    "    'camarilla_r3',\n",
    "    'camarilla_r4',\n",
    "    'camarilla_s1',\n",
    "    'camarilla_s2',\n",
    "    'camarilla_s3',\n",
    "    'camarilla_s4',\n",
    "    'cci',\n",
    "    'chaikinmoneyflow',\n",
    "    'chaikinoscillator',\n",
    "    'chaikinvolatility',\n",
    "    'choppinessindex',\n",
    "    'cmo',\n",
    "    'currentday_open',\n",
    "    'currentday_low',\n",
    "    'currentday_high',\n",
    "    'disparityindex',\n",
    "    'dm_diplus',\n",
    "    'dm_diminus',\n",
    "    'dmi',\n",
    "    'dmindex',\n",
    "    'donchian_lower',\n",
    "    'donchian_mean',\n",
    "    'donchian_upper',\n",
    "    'doublestochastics_k',\n",
    "    'easeofmovement',\n",
    "    'fibonacci_pp',\n",
    "    'fibonacci_r1',\n",
    "    'fibonacci_r2',\n",
    "    'fibonacci_r3',\n",
    "    'fibonacci_s1',\n",
    "    'fibonacci_s2',\n",
    "    'fibonacci_s3',\n",
    "    'fisherstransform',\n",
    "    'fosc',\n",
    "    'kama',\n",
    "    'keltner_lower',\n",
    "    'keltner_mean',\n",
    "    'keltner_upper',\n",
    "    'linreg',\n",
    "    'linregintercept',\n",
    "    'linregslope',\n",
    "    'macd',\n",
    "    'macd_avg',\n",
    "    'macd_diff',\n",
    "    'mama_default',\n",
    "    'mama_kama',\n",
    "    'mfi',\n",
    "    'momentum',\n",
    "    'moneyflowoscillator',\n",
    "    'orderflowcumulativedelta_deltaopen',\n",
    "    'orderflowcumulativedelta_deltaclose',\n",
    "    'orderflowcumulativedelta_deltahigh',\n",
    "    'orderflowcumulativedelta_deltalow',\n",
    "    'orderflowvwap_vwap',\n",
    "    'orderflowvwap_s1_lower',\n",
    "    'orderflowvwap_s1_higher',\n",
    "    'orderflowvwap_s2_lower',\n",
    "    'orderflowvwap_s2_higher',\n",
    "    'orderflowvwap_s3_lower',\n",
    "    'orderflowvwap_s3_higher',\n",
    "    'parabolic_sar',\n",
    "    'pfe',\n",
    "    'ppo',\n",
    "    'priceoscillator',\n",
    "    'psychologicalline',\n",
    "    'rsquared',\n",
    "    'relativevigorindex',\n",
    "    'rind',\n",
    "    'roc',\n",
    "    'rsi',\n",
    "    'rsi_avg',\n",
    "    'rss',\n",
    "    'rvi',\n",
    "    'stddev',\n",
    "    'stochrsi',\n",
    "    'stochastics_d',\n",
    "    'stochastics_k',\n",
    "    'stochasticsfast_d',\n",
    "    'stochasticsfast_k',\n",
    "    'trix',\n",
    "    'trix_signal',\n",
    "    'tsf',\n",
    "    'tsi',\n",
    "    'ultimateoscillator',\n",
    "    'vortex_viplus',\n",
    "    'vortex_viminus',\n",
    "    'volma',\n",
    "    'volume_oscillator',\n",
    "    'vroc',\n",
    "    'williamsr',\n",
    "    'wisemanawesomeoscillator',\n",
    "    'woodiescci',\n",
    "    'woodiescci_turbo',\n",
    "    'woodiespivot_pp',\n",
    "    'woodiespivot_r1',\n",
    "    'woodiespivot_r2',\n",
    "    'woodiespivot_s1',\n",
    "    'woodiespivot_s2'\n",
    "    ]\n",
    "\n",
    "group_helper = 'date' # This is the column that will be used to group the data and must be a datetime column in the format '%Y-%m-%d %H:%M:%S.%f'\n",
    "target_column = 'higherclose' # This is the column that will be used as the target column for the model and must be a binary column\n",
    "\n",
    "# Load the data from the input CSV file into a pandas dataframe\n",
    "df = pd.read_csv(file_in)\n",
    "\n",
    "# Convert 'date' column to datetime format for easier manipulation\n",
    "df[group_helper] = pd.to_datetime(df[group_helper], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "# Save the modified dataframe with only the specified columns to a new CSV file\n",
    "#df.to_csv(file_tmp, index=False, columns=columns_to_write)\n",
    "df[target_column] = df[target_column].astype(int)\n",
    "df_filtered = df[columns_to_write]\n",
    "#df_filtered.to_csv(file_tmp, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Normalize the Data**\n",
    "\n",
    "This step will use the MinMaxScaler to normalize the data.  The MinMaxScaler will scale the data to a range of 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# List of features to exclude from normalization\n",
    "features_to_exclude = [group_helper, target_column]\n",
    "\n",
    "# Dynamically select features to normalize (all features except the ones to exclude)\n",
    "features_to_normalize = [col for col in df_filtered.columns if col not in features_to_exclude]\n",
    "\n",
    "# Initialize the Scaler\n",
    "#scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler = RobustScaler() # AKA Z-score normalization, better at handling outliers\n",
    "\n",
    "# Fit the scaler to the data (for the features to be normalized)\n",
    "scaler.fit(df_filtered[features_to_normalize])\n",
    "\n",
    "# Transform the data using the fitted scaler\n",
    "df_normalized = df_filtered.copy()  # Create a copy of the DataFrame to keep the original data intact\n",
    "df_normalized[features_to_normalize] = scaler.transform(df_filtered[features_to_normalize])\n",
    "\n",
    "#df_normalized.to_csv(file_out, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: Create a Sequence Group Column Using a Rolling Window**\n",
    "\n",
    "This step will use a rolling window to create a sequence group column.  The sequence group column will be used to group sequences that are related to each other.  The rolling window will be based on a number of records.  The sequence groups will eliminate the need for padding sequences to the same length since the sequences will be grouped together and will contain exactly the same number of records specified by the \"rows_per_group\" variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2216/903613864.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_normalized[f'logical_{group_helper}'] = df_normalized[group_helper].dt.date\n"
     ]
    }
   ],
   "source": [
    "# STEP 4\n",
    "from datetime import datetime\n",
    "\n",
    "rows_per_group = 10 # The number of rows to include in each rolling window\n",
    "\n",
    "print(f\"DateTime: {datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}\")\n",
    "\n",
    "df_normalized[f'logical_{group_helper}'] = df_normalized[group_helper].dt.date\n",
    "# Initialize an empty DataFrame to hold the final results\n",
    "final_df = pd.DataFrame()\n",
    "\n",
    "# Loop over the DataFrame to create rolling windows\n",
    "for start in range(len(df_normalized) - rows_per_group):\n",
    "    window = df_normalized.iloc[start:start + rows_per_group]\n",
    "    # Check if all the dates in the window are the same\n",
    "    if len(set(window[f'logical_{group_helper}'])) == 1:\n",
    "        sequence_group = start + 1\n",
    "        # Check if the next record exists and is on the same logical date\n",
    "        if start + rows_per_group < len(df_normalized) and window.iloc[-1][f'logical_{group_helper}'] == df_normalized.iloc[start + rows_per_group][f'logical_{group_helper}']:\n",
    "            future_target = df_normalized.iloc[start + rows_per_group][target_column]\n",
    "        else:\n",
    "            future_target = None  # Set to None if there's no next record or it's on a different date\n",
    "        \n",
    "        window_copy = window.copy()\n",
    "        window_copy['sequence_group'] = sequence_group\n",
    "        window_copy[f'future_{target_column}'] = future_target\n",
    "        final_df = pd.concat([final_df, window_copy], ignore_index=True)\n",
    "\n",
    "# Filter out any sequence groups that don't have a future_target (indicating the next record was on a different day)\n",
    "final_df = final_df.dropna(subset=[f'future_{target_column}'])\n",
    "\n",
    "# Drop the date and convert all columns to float\n",
    "final_df = final_df.drop([group_helper,f'logical_{group_helper}'], axis=1)\n",
    "final_df = final_df.astype(float)\n",
    "\n",
    "# Write the final DataFrame to a new CSV file\n",
    "#print(len(final_df))\n",
    "#print(final_df.head())\n",
    "final_df.to_csv(file_tmp, index=False)\n",
    "print(f\"DateTime: {datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5: Shape the Data**\n",
    "Shape the data for RNN input, which requires a 3D shape [samples, timesteps, features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-04 23:52:48.409332: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-04 23:52:48.409503: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-04 23:52:48.427546: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-04 23:52:48.570311: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-04 23:52:50.741859: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Drop the 'sequence_group' column and separate features and labels\n",
    "X = final_df.drop(['sequence_group', f'future_{target_column}'], axis=1)\n",
    "y = final_df[f'future_{target_column}']\n",
    "\n",
    "# Since the data is already grouped, reshape it to fit the RNN input shape\n",
    "num_features = X.shape[1]\n",
    "num_sequences = len(final_df) // rows_per_group \n",
    "\n",
    "X_reshaped = X.values.reshape((num_sequences, rows_per_group, num_features))\n",
    "y_reshaped = y.values.reshape((num_sequences, rows_per_group))[:, 0]  # Take the first label of each sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6: Split the Data, Build, Compile, Train & Evaluate the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-04 23:52:53.192950: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-04 23:52:53.668400: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-04 23:52:53.668827: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-04 23:52:53.670526: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-04 23:52:53.671292: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-04 23:52:53.671787: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-04 23:52:53.934797: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-04 23:52:53.935273: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-04 23:52:53.935297: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-02-04 23:52:53.935664: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-04 23:52:53.935771: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6704 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "2024-02-04 23:52:54.884328: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-02-04 23:52:59.719361: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fb1b226aaf0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-02-04 23:52:59.719440: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1070, Compute Capability 6.1\n",
      "2024-02-04 23:52:59.729193: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-02-04 23:52:59.774125: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1707108779.883783    3310 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122/122 [==============================] - 1s 10ms/step\n",
      "RNN Model Evaluation Results:\n",
      "\n",
      "           Actual |             Predicted             |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 | not higherclose |   higherclose   |\n",
      "+-----------------+-----------------+-----------------+\n",
      "| not higherclose |      1167       |       754       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "| higherclose     |       717       |      1257       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "122/122 [==============================] - 1s 7ms/step\n",
      "LSTM Model Evaluation Results:\n",
      "\n",
      "           Actual |             Predicted             |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 | not higherclose |   higherclose   |\n",
      "+-----------------+-----------------+-----------------+\n",
      "| not higherclose |      1291       |       630       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "| higherclose     |       742       |      1232       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "122/122 [==============================] - 1s 7ms/step\n",
      "GRU Model Evaluation Results:\n",
      "\n",
      "           Actual |             Predicted             |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 | not higherclose |   higherclose   |\n",
      "+-----------------+-----------------+-----------------+\n",
      "| not higherclose |      1199       |       722       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "| higherclose     |       743       |      1231       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "122/122 [==============================] - 2s 9ms/step\n",
      "Stacked LSTM Model Evaluation Results:\n",
      "\n",
      "           Actual |             Predicted             |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 | not higherclose |   higherclose   |\n",
      "+-----------------+-----------------+-----------------+\n",
      "| not higherclose |      1234       |       687       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "| higherclose     |       682       |      1292       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "122/122 [==============================] - 2s 11ms/step\n",
      "Bidirectional LSTM Model Evaluation Results:\n",
      "\n",
      "           Actual |             Predicted             |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 | not higherclose |   higherclose   |\n",
      "+-----------------+-----------------+-----------------+\n",
      "| not higherclose |      1179       |       742       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "| higherclose     |       671       |      1303       |\n",
      "+-----------------+-----------------+-----------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-05 00:20:51.096010: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122/122 [==============================] - 1s 6ms/step\n",
      "1D Convolutional Neural Network (CNN) Model Evaluation Results:\n",
      "\n",
      "           Actual |             Predicted             |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 | not higherclose |   higherclose   |\n",
      "+-----------------+-----------------+-----------------+\n",
      "| not higherclose |      1921       |        0        |\n",
      "+-----------------+-----------------+-----------------+\n",
      "| higherclose     |      1974       |        0        |\n",
      "+-----------------+-----------------+-----------------+\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(type, model, X_test, y_test, target_column):\n",
    "    # Evaluate the model on the test set\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)  # Set verbose=0 to not print the evaluation log\n",
    "    #print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    # Generate model predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = (y_pred > 0.5).astype(\"int32\")  # Convert probabilities to binary predictions\n",
    "    \n",
    "    # Calculate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred_classes)\n",
    "    \n",
    "    # Labels for the confusion matrix\n",
    "    labels = [f\"not {target_column}\", f\"{target_column}\"]\n",
    "    \n",
    "    # Format and print the confusion matrix\n",
    "    header = f\"{'Actual |':>19} {'Predicted':^33} |\"\n",
    "    column_names = f\"| {'':>15} | {labels[0]:^15} | {labels[1]:^15} |\"\n",
    "    separator = \"+\" + \"-\"*17 + \"+\" + \"-\"*17 + \"+\" + \"-\"*17 + \"+\"\n",
    "    row1 = f\"| {labels[0]:<15} | {cm[0][0]:^15} | {cm[0][1]:^15} |\"\n",
    "    row2 = f\"| {labels[1]:<15} | {cm[1][0]:^15} | {cm[1][1]:^15} |\"\n",
    "    table = f\"\\n{header}\\n{separator}\\n{column_names}\\n{separator}\\n{row1}\\n{separator}\\n{row2}\\n{separator}\"\n",
    "    print(f\"{type} Model Evaluation Results:\")\n",
    "    print(table)\n",
    "\n",
    "def train_fit (type, model, X_train, y_train, X_test, y_test, target_column, set_epochs=10, set_batch_size=32):\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    # Fit the model to the training data\n",
    "    model.fit(X_train, y_train, epochs=set_epochs, batch_size=set_batch_size, validation_data=(X_test, y_test), verbose=0)\n",
    "    # Evaluate the model\n",
    "    evaluate_model(type, model, X_test, y_test, target_column)\n",
    "\n",
    "print(f\"DateTime: {datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}\")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_reshaped, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the RNN model\n",
    "model = Sequential([\n",
    "    SimpleRNN(50, input_shape=(rows_per_group, num_features), return_sequences=False),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "])\n",
    "train_fit(\"RNN\", model,X_train,y_train, X_test, y_test, target_column, 5, 1)\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(50, input_shape=(rows_per_group, num_features)),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "train_fit(\"LSTM\", model,X_train,y_train, X_test, y_test, target_column, 5, 1)\n",
    "\n",
    "# Build the GRU model\n",
    "model = Sequential([\n",
    "    GRU(50, input_shape=(rows_per_group, num_features)),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "train_fit(\"GRU\", model,X_train,y_train, X_test, y_test, target_column, 5, 1)\n",
    "\n",
    "# Build the Stacked LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(50, return_sequences=True, input_shape=(rows_per_group, num_features)),\n",
    "    Dropout(0.5),\n",
    "    LSTM(50),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "train_fit(\"Stacked LSTM\", model,X_train,y_train, X_test, y_test, target_column, 5, 1)\n",
    "\n",
    "# Build the Bidirectional LSTM model\n",
    "model = Sequential([\n",
    "    Bidirectional(LSTM(50), input_shape=(rows_per_group, num_features)),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "train_fit(\"Bidirectional LSTM\", model,X_train,y_train, X_test, y_test, target_column, 5, 1)\n",
    "\n",
    "# Build the 1D Convolutional Neural Network (CNN) model\n",
    "model = Sequential([\n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(rows_per_group, num_features)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(50, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "train_fit(\"1D Convolutional Neural Network (CNN)\", model,X_train,y_train, X_test, y_test, target_column, 5, 1)\n",
    "\n",
    "print(f\"DateTime: {datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
