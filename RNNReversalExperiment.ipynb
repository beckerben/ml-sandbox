{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reversal Experiment using Recurrent Neural Networks\n",
    "\n",
    "This notebook is meant to prepare a data file and conduct machine learning experiments using Neural Networks, specifically recurrent neural networks which support memory (RNN, LSTM, etc.).  The Neural Network models will be used for binary classification.\n",
    "\n",
    "The data file will be a CSV that is updated with a sequence group column and a label (target) column for a classification problem using machine learning models to predict the target.  The \"sequence_group\" column will be used to group sequences that are related to each other.  The label column will be used to indicate the target value for each sequence.  The data file will be used to train and test machine learning models to predict the target value for new sequences.\n",
    "\n",
    "The steps outlined here will build upon one another and should be run sequentially so that the final data file will be processed using a number of different Neural Network models.\n",
    "\n",
    "I highly recommend the use of a GPU for this experiment.  The use of a GPU will greatly reduce the time it takes to train the models.\n",
    "\n",
    "## File Preperation\n",
    "\n",
    "The basic steps for getting a file ready for the ML expermient are:\n",
    "\n",
    "1. Load the data file\n",
    "1. Filter the columns of interest which would include the features and the target (also known as the label)\n",
    "1. Normalize the data\n",
    "1. Create a sequence group column using a rolling window\n",
    "1. Shape the data\n",
    "1. Split the data, build, compile, train & evaluate the model\n",
    "1. Evaluate the model\n",
    "\n",
    "The data file has been created using NinjaTrader 8 and is a CSV file.  The rows represent renko bars.  The other features represent the indicators which were used with defaults.  Note: The indicators are not good features for this problem because they are being used on a chart type and at a granularity that is not typical for the indicator.  The indicators are being used to demonstrate the process of preparing the data file and conducting the machine learning experiments.  The utility which was used to create the data file is the \"Exporter\" strategy which I authored in my NinjaTrader repository on GitHub.  I have included a sample data file in the /data directory using NQ 30 tick renko bars.\n",
    "\n",
    "### Data File Assumptions\n",
    "\n",
    "- The data file will be a CSV\n",
    "- The data file will have a header row\n",
    "- The data file will have a column that contains the target value and it must be a binary value\n",
    "- The data file will be in sequential chronological order\n",
    "- The data file will have a column that contains a date and time value to aide sequence grouping\n",
    "- The data file is already cleansed with regard to missing values and outliers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps 1 & 2: Load the Data File and Filter the Columns of Interest**\n",
    "- Update the file paths\n",
    "- Update the column names for the features and the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1 & 2\n",
    "# Declare the input & output file paths, the columns to write and the target column\n",
    "# perform the imports\n",
    "import pandas as pd\n",
    "\n",
    "file_in = './data/sample.csv'\n",
    "file_tmp = './tmp/sample.csv'\n",
    "file_out = './data_prod/sample.csv'\n",
    "file_training = './data_prod/sample.csv'\n",
    "file_testing = './data_prod/sample.csv'\n",
    "\n",
    "columns_to_write = [\n",
    "    'date',\n",
    "    'higherclose',\n",
    "    'reversal',\n",
    "    'trendsequence',\n",
    "    'adl',\n",
    "    'adx',\n",
    "    'adxr',\n",
    "    'apz_lower',\n",
    "    'apz_upper',\n",
    "    'aroonoscillator',\n",
    "    'atr',\n",
    "    'bollinger_lower',\n",
    "    'bollinger_middle',\n",
    "    'bollinger_upper',\n",
    "    'bop',\n",
    "    'camarilla_r1',\n",
    "    'camarilla_r2',\n",
    "    'camarilla_r3',\n",
    "    'camarilla_r4',\n",
    "    'camarilla_s1',\n",
    "    'camarilla_s2',\n",
    "    'camarilla_s3',\n",
    "    'camarilla_s4',\n",
    "    'cci',\n",
    "    'chaikinmoneyflow',\n",
    "    'chaikinoscillator',\n",
    "    'chaikinvolatility',\n",
    "    'choppinessindex',\n",
    "    'cmo',\n",
    "    'currentday_open',\n",
    "    'currentday_low',\n",
    "    'currentday_high',\n",
    "    'disparityindex',\n",
    "    'dm_diplus',\n",
    "    'dm_diminus',\n",
    "    'dmi',\n",
    "    'dmindex',\n",
    "    'donchian_lower',\n",
    "    'donchian_mean',\n",
    "    'donchian_upper',\n",
    "    'doublestochastics_k',\n",
    "    'easeofmovement',\n",
    "    'fibonacci_pp',\n",
    "    'fibonacci_r1',\n",
    "    'fibonacci_r2',\n",
    "    'fibonacci_r3',\n",
    "    'fibonacci_s1',\n",
    "    'fibonacci_s2',\n",
    "    'fibonacci_s3',\n",
    "    'fisherstransform',\n",
    "    'fosc',\n",
    "    'kama',\n",
    "    'keltner_lower',\n",
    "    'keltner_mean',\n",
    "    'keltner_upper',\n",
    "    'linreg',\n",
    "    'linregintercept',\n",
    "    'linregslope',\n",
    "    'macd',\n",
    "    'macd_avg',\n",
    "    'macd_diff',\n",
    "    'mama_default',\n",
    "    'mama_kama',\n",
    "    'mfi',\n",
    "    'momentum',\n",
    "    'moneyflowoscillator',\n",
    "    'orderflowcumulativedelta_deltaopen',\n",
    "    'orderflowcumulativedelta_deltaclose',\n",
    "    'orderflowcumulativedelta_deltahigh',\n",
    "    'orderflowcumulativedelta_deltalow',\n",
    "    'orderflowvwap_vwap',\n",
    "    'orderflowvwap_s1_lower',\n",
    "    'orderflowvwap_s1_higher',\n",
    "    'orderflowvwap_s2_lower',\n",
    "    'orderflowvwap_s2_higher',\n",
    "    'orderflowvwap_s3_lower',\n",
    "    'orderflowvwap_s3_higher',\n",
    "    'parabolic_sar',\n",
    "    'pfe',\n",
    "    'ppo',\n",
    "    'priceoscillator',\n",
    "    'psychologicalline',\n",
    "    'rsquared',\n",
    "    'relativevigorindex',\n",
    "    'rind',\n",
    "    'roc',\n",
    "    'rsi',\n",
    "    'rsi_avg',\n",
    "    'rss',\n",
    "    'rvi',\n",
    "    'stddev',\n",
    "    'stochrsi',\n",
    "    'stochastics_d',\n",
    "    'stochastics_k',\n",
    "    'stochasticsfast_d',\n",
    "    'stochasticsfast_k',\n",
    "    'trix',\n",
    "    'trix_signal',\n",
    "    'tsf',\n",
    "    'tsi',\n",
    "    'ultimateoscillator',\n",
    "    'vortex_viplus',\n",
    "    'vortex_viminus',\n",
    "    'volma',\n",
    "    'volume_oscillator',\n",
    "    'vroc',\n",
    "    'williamsr',\n",
    "    'wisemanawesomeoscillator',\n",
    "    'woodiescci',\n",
    "    'woodiescci_turbo',\n",
    "    'woodiespivot_pp',\n",
    "    'woodiespivot_r1',\n",
    "    'woodiespivot_r2',\n",
    "    'woodiespivot_s1',\n",
    "    'woodiespivot_s2'\n",
    "    ]\n",
    "\n",
    "group_helper = 'date' # This is the column that will be used to group the data and must be a datetime column in the format '%Y-%m-%d %H:%M:%S.%f'\n",
    "target_column = 'reversal' # This is the column that will be used as the target column for the model and must be a binary column\n",
    "\n",
    "# Load the data from the input CSV file into a pandas dataframe\n",
    "df = pd.read_csv(file_in)\n",
    "\n",
    "# Convert 'date' column to datetime format for easier manipulation\n",
    "df[group_helper] = pd.to_datetime(df[group_helper], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "# Save the modified dataframe with only the specified columns to a new CSV file\n",
    "#df.to_csv(file_tmp, index=False, columns=columns_to_write)\n",
    "df[target_column] = df[target_column].astype(int)\n",
    "df_filtered = df[columns_to_write]\n",
    "#df_filtered.to_csv(file_tmp, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Normalize the Data**\n",
    "\n",
    "This step will use the MinMaxScaler to normalize the data.  The MinMaxScaler will scale the data to a range of 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# List of features to exclude from normalization\n",
    "features_to_exclude = [group_helper, target_column]\n",
    "\n",
    "# Dynamically select features to normalize (all features except the ones to exclude)\n",
    "features_to_normalize = [col for col in df_filtered.columns if col not in features_to_exclude]\n",
    "\n",
    "# Initialize the Scaler\n",
    "#scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler = RobustScaler() # AKA Z-score normalization, better at handling outliers\n",
    "\n",
    "# Fit the scaler to the data (for the features to be normalized)\n",
    "scaler.fit(df_filtered[features_to_normalize])\n",
    "\n",
    "# Transform the data using the fitted scaler\n",
    "df_normalized = df_filtered.copy()  # Create a copy of the DataFrame to keep the original data intact\n",
    "df_normalized[features_to_normalize] = scaler.transform(df_filtered[features_to_normalize])\n",
    "\n",
    "#df_normalized.to_csv(file_out, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: Create a Sequence Group Column Using a Rolling Window**\n",
    "\n",
    "This step will use a rolling window to create a sequence group column.  The sequence group column will be used to group sequences that are related to each other.  The rolling window will be based on a number of records.  The sequence groups will eliminate the need for padding sequences to the same length since the sequences will be grouped together and will contain exactly the same number of records specified by the \"rows_per_group\" variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14821/2797208004.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_normalized[f'logical_{group_helper}'] = df_normalized[group_helper].dt.date\n"
     ]
    }
   ],
   "source": [
    "# STEP 4\n",
    "rows_per_group = 5\n",
    "\n",
    "df_normalized[f'logical_{group_helper}'] = df_normalized[group_helper].dt.date\n",
    "# Initialize an empty DataFrame to hold the final results\n",
    "final_df = pd.DataFrame()\n",
    "\n",
    "# Loop over the DataFrame to create rolling windows\n",
    "for start in range(len(df_normalized) - rows_per_group):\n",
    "    window = df_normalized.iloc[start:start + rows_per_group]\n",
    "    # Check if all the dates in the window are the same\n",
    "    if len(set(window[f'logical_{group_helper}'])) == 1:\n",
    "        sequence_group = start + 1\n",
    "        # Check if the next record exists and is on the same logical date\n",
    "        if start + rows_per_group < len(df_normalized) and window.iloc[-1][f'logical_{group_helper}'] == df_normalized.iloc[start + rows_per_group][f'logical_{group_helper}']:\n",
    "            future_target = df_normalized.iloc[start + rows_per_group][target_column]\n",
    "        else:\n",
    "            future_target = None  # Set to None if there's no next record or it's on a different date\n",
    "        \n",
    "        window_copy = window.copy()\n",
    "        window_copy['sequence_group'] = sequence_group\n",
    "        window_copy[f'future_{target_column}'] = future_target\n",
    "        final_df = pd.concat([final_df, window_copy], ignore_index=True)\n",
    "\n",
    "# Filter out any sequence groups that don't have a future_reversal (indicating the next record was on a different day)\n",
    "final_df = final_df.dropna(subset=[f'future_{target_column}'])\n",
    "\n",
    "# Drop the date and convert all columns to float\n",
    "final_df = final_df.drop([group_helper,f'logical_{group_helper}'], axis=1)\n",
    "final_df = final_df.astype(float)\n",
    "\n",
    "# Write the final DataFrame to a new CSV file\n",
    "#print(len(final_df))\n",
    "#print(final_df.head())\n",
    "final_df.to_csv(file_tmp, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5: Shape the Data**\n",
    "Shape the data for RNN input, which requires a 3D shape [samples, timesteps, features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Drop the 'sequence_group' column and separate features and labels\n",
    "X = final_df.drop(['sequence_group', f'future_{target_column}'], axis=1)\n",
    "y = final_df[f'future_{target_column}']\n",
    "\n",
    "# Since the data is already grouped, reshape it to fit the RNN input shape\n",
    "num_features = X.shape[1]\n",
    "num_sequences = len(final_df) // rows_per_group \n",
    "\n",
    "X_reshaped = X.values.reshape((num_sequences, rows_per_group, num_features))\n",
    "y_reshaped = y.values.reshape((num_sequences, rows_per_group))[:, 0]  # Take the first label of each sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6: Split the Data, Build, Compile, Train & Evaluate the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126/126 [==============================] - 1s 6ms/step\n",
      "RNN Model Evaluation Results:\n",
      "\n",
      "           Actual |             Predicted             |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |  not reversal   |    reversal     |\n",
      "+-----------------+-----------------+-----------------+\n",
      "| not reversal    |      2272       |       386       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "| reversal        |      1154       |       210       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "126/126 [==============================] - 1s 5ms/step\n",
      "LSTM Model Evaluation Results:\n",
      "\n",
      "           Actual |             Predicted             |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |  not reversal   |    reversal     |\n",
      "+-----------------+-----------------+-----------------+\n",
      "| not reversal    |      2652       |        6        |\n",
      "+-----------------+-----------------+-----------------+\n",
      "| reversal        |      1363       |        1        |\n",
      "+-----------------+-----------------+-----------------+\n",
      "126/126 [==============================] - 2s 6ms/step\n",
      "GRU Model Evaluation Results:\n",
      "\n",
      "           Actual |             Predicted             |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |  not reversal   |    reversal     |\n",
      "+-----------------+-----------------+-----------------+\n",
      "| not reversal    |      2227       |       431       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "| reversal        |      1167       |       197       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "126/126 [==============================] - 3s 9ms/step\n",
      "Stacked LSTM Model Evaluation Results:\n",
      "\n",
      "           Actual |             Predicted             |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |  not reversal   |    reversal     |\n",
      "+-----------------+-----------------+-----------------+\n",
      "| not reversal    |      2658       |        0        |\n",
      "+-----------------+-----------------+-----------------+\n",
      "| reversal        |      1364       |        0        |\n",
      "+-----------------+-----------------+-----------------+\n",
      "126/126 [==============================] - 5s 25ms/step\n",
      "Bidirectional LSTM Model Evaluation Results:\n",
      "\n",
      "           Actual |             Predicted             |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |  not reversal   |    reversal     |\n",
      "+-----------------+-----------------+-----------------+\n",
      "| not reversal    |      2568       |       90        |\n",
      "+-----------------+-----------------+-----------------+\n",
      "| reversal        |      1323       |       41        |\n",
      "+-----------------+-----------------+-----------------+\n",
      "126/126 [==============================] - 1s 6ms/step\n",
      "1D Convolutional Neural Network (CNN) Model Evaluation Results:\n",
      "\n",
      "           Actual |             Predicted             |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |  not reversal   |    reversal     |\n",
      "+-----------------+-----------------+-----------------+\n",
      "| not reversal    |      2655       |        3        |\n",
      "+-----------------+-----------------+-----------------+\n",
      "| reversal        |      1364       |        0        |\n",
      "+-----------------+-----------------+-----------------+\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(type, model, X_test, y_test, target_column):\n",
    "    # Evaluate the model on the test set\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)  # Set verbose=0 to not print the evaluation log\n",
    "    #print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    # Generate model predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = (y_pred > 0.5).astype(\"int32\")  # Convert probabilities to binary predictions\n",
    "    \n",
    "    # Calculate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred_classes)\n",
    "    \n",
    "    # Labels for the confusion matrix\n",
    "    labels = [f\"not {target_column}\", f\"{target_column}\"]\n",
    "    \n",
    "    # Format and print the confusion matrix\n",
    "    header = f\"{'Actual |':>19} {'Predicted':^33} |\"\n",
    "    column_names = f\"| {'':>15} | {labels[0]:^15} | {labels[1]:^15} |\"\n",
    "    separator = \"+\" + \"-\"*17 + \"+\" + \"-\"*17 + \"+\" + \"-\"*17 + \"+\"\n",
    "    row1 = f\"| {labels[0]:<15} | {cm[0][0]:^15} | {cm[0][1]:^15} |\"\n",
    "    row2 = f\"| {labels[1]:<15} | {cm[1][0]:^15} | {cm[1][1]:^15} |\"\n",
    "    table = f\"\\n{header}\\n{separator}\\n{column_names}\\n{separator}\\n{row1}\\n{separator}\\n{row2}\\n{separator}\"\n",
    "    print(f\"{type} Model Evaluation Results:\")\n",
    "    print(table)\n",
    "\n",
    "def train_fit (type, model, X_train, y_train, X_test, y_test, target_column, set_epochs=10, set_batch_size=32):\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    # Fit the model to the training data\n",
    "    model.fit(X_train, y_train, epochs=set_epochs, batch_size=set_batch_size, validation_data=(X_test, y_test), verbose=0)\n",
    "    # Evaluate the model\n",
    "    evaluate_model(type, model, X_test, y_test, target_column)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_reshaped, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the RNN model\n",
    "model = Sequential([\n",
    "    SimpleRNN(50, input_shape=(rows_per_group, num_features), return_sequences=False),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "])\n",
    "train_fit(\"RNN\", model,X_train,y_train, X_test, y_test, target_column, 1, 1)\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(50, input_shape=(rows_per_group, num_features)),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "train_fit(\"LSTM\", model,X_train,y_train, X_test, y_test, target_column, 1, 1)\n",
    "\n",
    "# Build the GRU model\n",
    "model = Sequential([\n",
    "    GRU(50, input_shape=(rows_per_group, num_features)),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "train_fit(\"GRU\", model,X_train,y_train, X_test, y_test, target_column, 1, 1)\n",
    "\n",
    "# Build the Stacked LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(50, return_sequences=True, input_shape=(rows_per_group, num_features)),\n",
    "    Dropout(0.5),\n",
    "    LSTM(50),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "train_fit(\"Stacked LSTM\", model,X_train,y_train, X_test, y_test, target_column, 1, 1)\n",
    "\n",
    "# Build the Bidirectional LSTM model\n",
    "model = Sequential([\n",
    "    Bidirectional(LSTM(50), input_shape=(rows_per_group, num_features)),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "train_fit(\"Bidirectional LSTM\", model,X_train,y_train, X_test, y_test, target_column, 1, 1)\n",
    "\n",
    "# Build the 1D Convolutional Neural Network (CNN) model\n",
    "model = Sequential([\n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(rows_per_group, num_features)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(50, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "train_fit(\"1D Convolutional Neural Network (CNN)\", model,X_train,y_train, X_test, y_test, target_column, 1, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
